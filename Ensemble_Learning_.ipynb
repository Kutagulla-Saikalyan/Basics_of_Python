{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Learning:\n"
      ],
      "metadata": {
        "id": "2EMOzy0fqEMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is ensemble learning and why is it used in machine learning?\n",
        "\n",
        "> Ensemble learning is a machine learning technique that combines the predictions of multiple individual models to improve the overall prediction performance. The goal of ensemble learning is to create a more robust and accurate predictive model by combining the strengths of multiple base models.\n",
        "\n",
        "> Ensemble learning is used in machine learning because it has been shown to improve the accuracy and stability of predictions compared to using a single model. This is achieved by combining the diverse predictions of multiple base models, which reduces the chances of overfitting and increases the robustness of the overall model. By combining the strengths of different models, ensemble learning can also help to reduce the impact of biases or weaknesses in individual models, leading to a more balanced and accurate prediction.\n",
        "\n",
        "> **For example**, in a binary classification problem, suppose we have two individual models: Model A, which is good at correctly classifying positive examples but struggles with negative examples, and Model B, which is good at correctly classifying negative examples but struggles with positive examples. By combining the predictions of these two models, we can create a more accurate ensemble model that outperforms either individual model alone.\n",
        "\n",
        "> **Another example** is in image classification, where an ensemble of convolutional neural networks (CNNs) can be trained on the same image dataset and their predictions combined to produce a more accurate final classification.\n",
        "\n",
        "There are various ensemble learning methods, such as bagging, boosting, random forest, and stacking, each with its own strengths and weaknesses. The choice of method will depend on the specific requirements of the problem, the data, and the resources available."
      ],
      "metadata": {
        "id": "P5r4UKBkqEXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of Ensemble Learning:\n",
        "\n",
        "> **Increased Accuracy:** Ensemble learning can often lead to improved accuracy compared to using a single model, by combining the strengths of multiple base models and reducing the impact of biases or weaknesses in individual models.\n",
        "\n",
        "> **Reduced Overfitting:** Ensemble learning can help to reduce the overfitting of individual models, by combining the diverse predictions of multiple base models and reducing the chances of overfitting to the training data.\n",
        "\n",
        "> **Increased Robustness:** Ensemble learning can increase the robustness of predictions, by combining the diverse predictions of multiple base models and reducing the impact of noise or outliers in the data.\n",
        "\n",
        "> **Improved Interpretability:** Ensemble learning can also provide improved interpretability, by allowing the contribution of individual base models to the final prediction to be analyzed and understood.\n",
        "\n",
        "### Disadvantages of Ensemble Learning:\n",
        "\n",
        "> **Increased Complexity:** Ensemble learning can lead to increased complexity, as multiple base models must be trained and combined, and the choice of base models and combination method must be carefully considered.\n",
        "\n",
        "> **Increased Computational Cost:** Ensemble learning can also lead to increased computational cost, as multiple base models must be trained and combined, which can be computationally expensive.\n",
        "\n",
        "> **Difficulty in Training:** Training ensemble models can be difficult, as the choice of base models, combination method, and hyperparameters must be carefully considered in order to achieve optimal performance."
      ],
      "metadata": {
        "id": "G3sjQ6Z7uGhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of ensemble learning methods: \n",
        "*  Bagging, \n",
        "*  Boosting,\n",
        "*  Stacking,\n",
        "*  Random Forest,\n",
        "*  AdaBoost."
      ],
      "metadata": {
        "id": "DL_RHHk2qEan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging (Bootstrapped Aggregation):\n",
        "> **Bagging, or Bootstrapped Aggregation,** is a simple ensemble learning method that is used to improve the stability and accuracy of machine learning models by combining the predictions of multiple base models. Bagging works by training multiple base models on different random subsets of the training data, and then combining their predictions to produce the final prediction."
      ],
      "metadata": {
        "id": "6XboAzRZvCGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple example of a binary classification dataset:\n",
        "import pandas as pd\n",
        "\n",
        "# Creating the dataset\n",
        "data = {'Feature1': [0.5, 0.7, 0.6, 0.9, 0.8],\n",
        "        'Feature2': [0.3, 0.4, 0.5, 0.7, 0.6],\n",
        "        'Target': [0, 1, 1, 0, 1]}\n",
        "\n",
        "# Converting the data into a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Printing the first five rows of the data\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDpkplwW1bPU",
        "outputId": "981a7cc8-4be2-4177-8622-42207ba0b8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature1  Feature2  Target\n",
            "0       0.5       0.3       0\n",
            "1       0.7       0.4       1\n",
            "2       0.6       0.5       1\n",
            "3       0.9       0.7       0\n",
            "4       0.8       0.6       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is an example of bagging in a real-time project using Python and the scikit-learn library:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.DataFrame(df)\n",
        "X = data.drop(\"Target\", axis=1)\n",
        "y = data[\"Target\"]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a bagging classifier using decision trees as base models\n",
        "base_model = DecisionTreeClassifier(max_depth=4)\n",
        "bagging_model = BaggingClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CSJMm2ruFdo",
        "outputId": "c075e25e-5c6d-466b-e597-b390fa90c22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Above example, we first load the data into a pandas DataFrame, and then split the data into training and test sets. Next, we train a bagging classifier using decision trees as the base models. The BaggingClassifier class from scikit-learn is used to train the bagging model. In this example, we specify that 50 base models should be trained and combined, and the random seed is set to 42 to ensure reproducibility. Finally, we make predictions on the test set, evaluate the performance of the model using the accuracy score, and print the result.\n",
        "\n",
        "Bagging is a simple and effective ensemble learning method that can be used to improve the performance of machine learning models, especially in cases where the individual base models have high variance or overfit to the training data."
      ],
      "metadata": {
        "id": "TLoWPpmc2OUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting: \n",
        "> **Boosting** is an ensemble learning method that trains multiple models sequentially, with each model learning from the mistakes made by previous models. The aim of boosting is to improve the performance of a weak model by combining it with multiple other weak models.\n",
        "\n",
        "> Simple example of how Boosting works in a real-time project using the Python library xgboost. This example uses the Iris dataset, which is a classic dataset in machine learning and consists of 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. The goal is to predict the species of the iris flowers based on these four features."
      ],
      "metadata": {
        "id": "c-P-ytv33Gjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creating a Boosting model\n",
        "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "5_seJuqczKY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914cdd29-e629-4f88-e08d-90c255cfd45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Above example, an XGBClassifier model is created with 100 trees and a learning rate of 0.1. The model is trained on the training set and evaluated on the test set. The accuracy of the model is printed as the output, which gives you an idea of how well the Boosting model is able to predict the species of iris flowers based on the four features.\n",
        "\n",
        "Note that this is a very simple example, and in real-world projects, you would typically perform several other steps, such as data preprocessing, feature selection, hyperparameter tuning, etc., before arriving at the final model."
      ],
      "metadata": {
        "id": "pcV__wCo313D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking:\n",
        "\n",
        "> **Stacking** is an ensemble learning method that combines the predictions of multiple models to produce a final prediction. Unlike Boosting, which trains models sequentially, Stacking trains models in parallel and then combines their predictions using a meta-model.\n",
        "\n",
        "> Simple example of how Stacking works in a real-time project using the Python library scikit-learn. This example uses the Iris dataset, which is a classic dataset in machine learning and consists of 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. The goal is to predict the species of the iris flowers based on these four features."
      ],
      "metadata": {
        "id": "p0Ha6TZB4hL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creating base models\n",
        "base_models = [RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "               LogisticRegression(random_state=0)]\n",
        "\n",
        "# Creating a list to store the predictions of the base models\n",
        "base_predictions = []\n",
        "\n",
        "# Loop through the base models and get their predictions on the training set\n",
        "for base_model in base_models:\n",
        "    base_model.fit(X_train, y_train)\n",
        "    base_predictions.append(base_model.predict_proba(X_train))\n",
        "\n",
        "# Stack the base model predictions using a meta-model\n",
        "meta_model = LogisticRegression(random_state=0)\n",
        "meta_model.fit(np.hstack(base_predictions), y_train)\n",
        "\n",
        "# Use the meta-model to make predictions on the test set\n",
        "y_pred = meta_model.predict(np.hstack([base_model.predict_proba(X_test) for base_model in base_models]))\n",
        "\n",
        "# Evaluate the accuracy of the Stacking model\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5KrKHbq3SBq",
        "outputId": "d167bb17-035d-42b6-f66d-6e76d6e7ea55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Above example, two base models, a RandomForestClassifier and a LogisticRegression model, are trained on the training set. The predictions of the two base models are then stacked using a LogisticRegression meta-model. The final prediction is made by the meta-model, and the accuracy of the Stacking model is evaluated on the test set.\n",
        "\n",
        "Note that this is a very simple example, and in real-world projects, you would typically perform several other steps, such as data preprocessing, feature selection, hyperparameter tuning, etc., before arriving at the final model."
      ],
      "metadata": {
        "id": "zpyEyvgC49zF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest:\n",
        "\n",
        "> **Random Forest** is an ensemble learning method that combines multiple decision trees to produce a final prediction. In a Random Forest model, each decision tree is trained on a different random subset of the training data, and the final prediction is made by averaging the predictions of all the trees. This helps reduce overfitting and improves the model's ability to generalize to new data.\n",
        "\n",
        "> Simple example of how Random Forest works in a real-time project using the Python library scikit-learn. This example uses the Iris dataset, which is a classic dataset in machine learning and consists of 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. The goal is to predict the species of the iris flowers based on these four features.\n"
      ],
      "metadata": {
        "id": "4JMJ5Y_66kOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creating a Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "\n",
        "# Training the Random Forest classifier on the training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of the Random Forest classifier\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRvv3JHl42os",
        "outputId": "91020f88-3916-469e-edcc-7a251d8c1528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Above example, a RandomForestClassifier is created and trained on the training set. The accuracy of the model is then evaluated on the test set. The n_estimators parameter specifies the number of decision trees in the Random Forest. Increasing the value of n_estimators will typically lead to better performance, but will also increase the computational cost of the model.\n",
        "\n",
        "Note that this is a very simple example, and in real-world projects, you would typically perform several other steps, such as data preprocessing, feature selection, hyperparameter tuning, etc., before arriving at the final model."
      ],
      "metadata": {
        "id": "o2JP5EK666MH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost:\n",
        "> **AdaBoost (Adaptive Boosting)** is an ensemble learning method that adjusts the weights of individual samples in the training data based on their performance in previous rounds of training. The goal of AdaBoost is to give more weight to the samples that are difficult to predict, so that the final classifier is able to better capture the underlying patterns in the data.\n",
        "\n",
        "> Simple example of how AdaBoost works in a real-time project using the Python library scikit-learn. This example uses the Iris dataset, which is a classic dataset in machine learning and consists of 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. The goal is to predict the species of the iris flowers based on these four features."
      ],
      "metadata": {
        "id": "cSKcsd9P9R0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Creating an AdaBoost classifier\n",
        "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "\n",
        "# Training the AdaBoost classifier on the training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of the AdaBoost classifier\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkBSWzK5630i",
        "outputId": "2cf4a098-ceed-420b-ca11-04d9bda62854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Above example, an AdaBoostClassifier is created and trained on the training set. The accuracy of the model is then evaluated on the test set. The n_estimators parameter specifies the number of weak classifiers in the AdaBoost. Increasing the value of n_estimators will typically lead to better performance, but will also increase the computational cost of the model.\n",
        "\n",
        "Note that this is a very simple example, and in real-world projects, you would typically perform several other steps, such as data preprocessing, feature selection, hyperparameter tuning, etc., before arriving at the final model."
      ],
      "metadata": {
        "id": "PiyXYsYf-BBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion:\n",
        "> Ensemble learning method to use depends on the specific problem and dataset at hand. Here's a brief overview of when you might use each method:\n",
        "\n",
        "*  **Bagging:** Bagging is a simple and effective way to reduce overfitting and improve the generalization performance of a single base classifier. Bagging works well when the base classifiers are unstable (i.e., have high variance), but are still somewhat accurate.\n",
        "\n",
        "*  **Boosting:** Boosting is a more sophisticated ensemble learning method that focuses on training a sequence of weak classifiers and combining their outputs in a weighted manner. Boosting is particularly useful when the base classifiers are unstable, but have low bias.\n",
        "\n",
        "*  **Stacking:** Stacking is a more complex ensemble learning method that involves training a second-level classifier to make predictions based on the predictions of the base classifiers. Stacking can be used to combine the strengths of different types of base classifiers and can often lead to improved performance compared to other ensemble methods.\n",
        "\n",
        "*  **Random Forest:** Random Forest is an ensemble learning method that combines decision trees as base classifiers and uses bagging and feature randomization to reduce overfitting and improve generalization performance. Random Forest works well on a wide range of problems, including both regression and classification tasks.\n",
        "\n",
        "*  **AdaBoost:** AdaBoost is a boosting method that adjusts the weights of the samples in the training data based on their performance in previous rounds of training. AdaBoost is often used for binary classification problems and can be a good choice when the base classifiers are simple and have low bias.\n",
        "\n",
        "Ensemble learning method, as the best method will depend on the specific problem and dataset at hand. It is common to try several different ensemble methods and compare their performance on the task at hand to determine which method works best. Additionally, ensembles can often be combined to further improve performance, for example, by using a Random Forest as the base classifier in a stacking ensemble."
      ],
      "metadata": {
        "id": "Ap-iNol__aZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <div class=\"alert alert-block alert-success\"> <span  style= \"font-family: Times New Roman\">**Happy learning!** </span> </div>"
      ],
      "metadata": {
        "id": "x0e83hz7BQTC"
      }
    }
  ]
}